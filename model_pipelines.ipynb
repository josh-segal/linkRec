{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b747ada",
      "metadata": {},
      "source": [
        "# ## Data Class for Logging Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "003968a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "@dataclass\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Stores training metrics for either DQN or Contextual Bandit\"\"\"\n",
        "    timesteps: List[int] = field(default_factory=list)\n",
        "    rewards: List[float] = field(default_factory=list)\n",
        "    cumulative_rewards: List[float] = field(default_factory=list)\n",
        "    average_rewards: List[float] = field(default_factory=list)\n",
        "    losses: List[Optional[float]] = field(default_factory=list)\n",
        "    exploration_rates: List[Optional[float]] = field(default_factory=list)\n",
        "\n",
        "class MetricsLogger:\n",
        "    \"\"\"Handles logging of training metrics for both algorithms\"\"\"\n",
        "    def __init__(self, log_interval: int = 100):\n",
        "        self.log_interval = log_interval\n",
        "        self.metrics = defaultdict(TrainingMetrics)\n",
        "        self.cum_rewards = defaultdict(float)\n",
        "        \n",
        "    def log_step(self, \n",
        "                 algorithm: str,\n",
        "                 timestep: int, \n",
        "                 reward: float,\n",
        "                 loss: Optional[float] = None,\n",
        "                 exploration_rate: Optional[float] = None):\n",
        "        \"\"\"Log metrics for a single training step\"\"\"\n",
        "        \n",
        "        # Update cumulative reward\n",
        "        self.cum_rewards[algorithm] += reward\n",
        "        \n",
        "        # Only log at specified intervals\n",
        "        if timestep % self.log_interval == 0:\n",
        "            metrics = self.metrics[algorithm]\n",
        "            metrics.timesteps.append(timestep)\n",
        "            metrics.rewards.append(reward)\n",
        "            metrics.cumulative_rewards.append(self.cum_rewards[algorithm])\n",
        "            metrics.average_rewards.append(self.cum_rewards[algorithm] / (timestep + 1))\n",
        "            metrics.losses.append(loss)\n",
        "            metrics.exploration_rates.append(exploration_rate)\n",
        "\n",
        "def plot_comparison(logger: MetricsLogger, \n",
        "                   save_path: Optional[str] = None,\n",
        "                   figsize: tuple = (15, 10),\n",
        "                   window_size: int = 50):\n",
        "    \"\"\"Plot comparison of DQN vs Contextual Bandit performance\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
        "    fig.suptitle('DQN vs Contextual Bandit Performance Comparison')\n",
        "    \n",
        "    # Plot settings\n",
        "    algorithms = ['DQN', 'Contextual Bandit']\n",
        "    colors = ['blue', 'red']\n",
        "    \n",
        "    # Reward plots\n",
        "    ax = axes[0, 0]\n",
        "    for alg, color in zip(algorithms, colors):\n",
        "        metrics = logger.metrics[alg]\n",
        "        if len(metrics.rewards) > 0:  # Check if we have data\n",
        "            # Plot raw rewards\n",
        "            ax.plot(metrics.timesteps, metrics.rewards, color=color, label=f'{alg} (raw)', alpha=0.3)\n",
        "            \n",
        "            # Calculate and plot rolling average only if we have enough data\n",
        "            if len(metrics.rewards) >= window_size:\n",
        "                rolling_mean = np.convolve(metrics.rewards, \n",
        "                                         np.ones(window_size)/window_size, \n",
        "                                         mode='valid')\n",
        "                # Adjust timesteps to match rolling mean length\n",
        "                valid_timesteps = metrics.timesteps[window_size-1:window_size-1+len(rolling_mean)]\n",
        "                ax.plot(valid_timesteps, rolling_mean, \n",
        "                       color=color, label=f'{alg} (moving avg)')\n",
        "    ax.set_title('Rewards per Step')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Cumulative reward\n",
        "    ax = axes[0, 1]\n",
        "    for alg, color in zip(algorithms, colors):\n",
        "        metrics = logger.metrics[alg]\n",
        "        if len(metrics.cumulative_rewards) > 0:\n",
        "            ax.plot(metrics.timesteps, metrics.cumulative_rewards, \n",
        "                   color=color, label=alg)\n",
        "    ax.set_title('Cumulative Reward')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('Cumulative Reward')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Average reward\n",
        "    ax = axes[1, 0]\n",
        "    for alg, color in zip(algorithms, colors):\n",
        "        metrics = logger.metrics[alg]\n",
        "        if len(metrics.average_rewards) > 0:\n",
        "            ax.plot(metrics.timesteps, metrics.average_rewards, \n",
        "                   color=color, label=alg)\n",
        "    ax.set_title('Average Reward')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('Average Reward')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Loss/Exploration (DQN only)\n",
        "    ax = axes[1, 1]\n",
        "    metrics = logger.metrics['DQN']\n",
        "    if len(metrics.losses) > 0:\n",
        "        # Filter out None values for losses\n",
        "        valid_losses = [(t, l) for t, l in zip(metrics.timesteps, metrics.losses) if l is not None]\n",
        "        if valid_losses:\n",
        "            timesteps_loss, losses = zip(*valid_losses)\n",
        "            ax.plot(timesteps_loss, losses, color='blue', label='Loss')\n",
        "    \n",
        "    ax2 = ax.twinx()\n",
        "    if len(metrics.exploration_rates) > 0:\n",
        "        # Filter out None values for exploration rates\n",
        "        valid_exp = [(t, e) for t, e in zip(metrics.timesteps, metrics.exploration_rates) \n",
        "                     if e is not None]\n",
        "        if valid_exp:\n",
        "            timesteps_exp, exp_rates = zip(*valid_exp)\n",
        "            ax2.plot(timesteps_exp, exp_rates, color='green', label='Exploration Rate')\n",
        "    \n",
        "    ax.set_title('DQN Loss and Exploration Rate')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax2.set_ylabel('Exploration Rate')\n",
        "    \n",
        "    # Combine legends for the last subplot\n",
        "    lines1, labels1 = ax.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    if lines1 or lines2:\n",
        "        ax.legend(lines1 + lines2, labels1 + labels2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Data Loading Functions\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(sample_fraction=0.01, random_state=42):\n",
        "    \"\"\"\n",
        "    Load news and behaviors data from MIND dataset with optional sampling\n",
        "    \n",
        "    Args:\n",
        "        sample_fraction: fraction of behaviors data to sample (default: 0.01)\n",
        "        random_state: random seed for reproducibility (default: 42)\n",
        "        \n",
        "    Returns:\n",
        "        news_df: DataFrame containing news articles\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "    \"\"\"\n",
        "    # Read news data\n",
        "    col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
        "    news_df = pd.read_csv('MINDsmall_train/news.tsv', sep='\\t', header=None, names=col_news)\n",
        "    \n",
        "    # Read behaviors data\n",
        "    col_behaviors = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
        "    behaviors_df = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None, names=col_behaviors)\n",
        "    behaviors_df = behaviors_df.sample(frac=sample_fraction, random_state=random_state)\n",
        "    \n",
        "    return news_df, behaviors_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Common Utility Functions\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_time_features(time_str):\n",
        "    \"\"\"\n",
        "    Extract time-based features from timestamp\n",
        "    \n",
        "    Args:\n",
        "        time_str: timestamp string in format '%m/%d/%Y %I:%M:%S %p'\n",
        "        \n",
        "    Returns:\n",
        "        dict: containing hour, day_of_week, is_weekend features\n",
        "    \"\"\"\n",
        "    time = datetime.strptime(time_str, '%m/%d/%Y %I:%M:%S %p')\n",
        "    return {\n",
        "        'hour': time.hour,\n",
        "        'day_of_week': time.weekday(),\n",
        "        'is_weekend': 1 if time.weekday() >= 5 else 0\n",
        "    }\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_categorical_features(news_df):\n",
        "    \"\"\"\n",
        "    One-hot encode news category and subcategory\n",
        "    \n",
        "    Args:\n",
        "        news_df: DataFrame containing news articles\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: news_df with appended one-hot encoded features\n",
        "    \"\"\"\n",
        "    # One-hot encode category and subcategory\n",
        "    category_ohe = pd.get_dummies(news_df['Category'], prefix='cat')\n",
        "    subcategory_ohe = pd.get_dummies(news_df['SubCat'], prefix='subcat')\n",
        "\n",
        "    # Concatenate one-hot columns to news_df\n",
        "    return pd.concat([news_df, category_ohe, subcategory_ohe], axis=1)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Data Processing for Different Models\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_impressions_for_classification(behaviors_df, news_df):\n",
        "    \"\"\"\n",
        "    Convert behaviors data into user-news interaction pairs for classification\n",
        "    with balanced classes through downsampling\n",
        "    \n",
        "    Args:\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "        news_df: DataFrame containing news articles\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: balanced interactions for classification task\n",
        "    \"\"\"\n",
        "    interactions = []\n",
        "    \n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        user_id = row['UserID']\n",
        "        history = row['History'].split() if pd.notna(row['History']) else []\n",
        "        \n",
        "        # Process each impression\n",
        "        for impression in row['Impressions'].split():\n",
        "            news_id, click = impression.split('-')\n",
        "            \n",
        "            # Only include if news exists in news_df\n",
        "            if news_id in news_df['NewsId'].values:\n",
        "                interactions.append({\n",
        "                    'user_id': user_id,\n",
        "                    'news_id': news_id,\n",
        "                    'click': int(click),\n",
        "                    'history_len': len(history),\n",
        "                    'time': row['Time']\n",
        "                })\n",
        "\n",
        "    # Convert interactions list to DataFrame\n",
        "    interactions_df = pd.DataFrame(interactions)\n",
        "    \n",
        "    # Separate clicks and no-clicks\n",
        "    clicks = interactions_df[interactions_df['click'] == 1]\n",
        "    no_clicks = interactions_df[interactions_df['click'] == 0]\n",
        "    \n",
        "    # Downsample no_clicks to match clicks size\n",
        "    no_clicks_downsampled = no_clicks.sample(n=len(clicks), random_state=42)\n",
        "    \n",
        "    # Combine back\n",
        "    balanced_df = pd.concat([clicks, no_clicks_downsampled])\n",
        "    \n",
        "    print(f\"Original size: {len(interactions_df)}, Balanced size: {len(balanced_df)}\")\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    print(balanced_df['click'].value_counts())\n",
        "    \n",
        "    return balanced_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_for_classification(interactions_df, news_df):\n",
        "    \"\"\"\n",
        "    Prepare features for classification task\n",
        "    \n",
        "    Args:\n",
        "        interactions_df: DataFrame with user-news interactions\n",
        "        news_df: DataFrame with one-hot encoded news features\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: features ready for classification\n",
        "    \"\"\"\n",
        "    # Identify one-hot columns to include\n",
        "    category_ohe = [col for col in news_df.columns if col.startswith('cat_')]\n",
        "    subcategory_ohe = [col for col in news_df.columns if col.startswith('subcat_')]\n",
        "\n",
        "    # Merge only necessary columns (include one-hot columns)\n",
        "    merged_news_cols = ['NewsId'] + category_ohe + subcategory_ohe\n",
        "\n",
        "    # Merge news features with interactions\n",
        "    features_df = interactions_df.merge(\n",
        "        news_df[merged_news_cols],\n",
        "        left_on='news_id',\n",
        "        right_on='NewsId'\n",
        "    )\n",
        "    \n",
        "    # Add time features\n",
        "    time_features = features_df['time'].apply(extract_time_features).apply(pd.Series)\n",
        "    features_df = pd.concat([features_df, time_features], axis=1)\n",
        "    \n",
        "    return features_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data_for_bandit_and_dqn(behaviors_df, news_df_encoded):\n",
        "    \"\"\"\n",
        "    Process data into format suitable for both LinUCB bandit and DQN models\n",
        "    \n",
        "    Args:\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "        news_df_encoded: DataFrame containing news with encoded features\n",
        "        \n",
        "    Returns:\n",
        "        list: interactions in bandit/DQN format\n",
        "        DataFrame: news features for reference\n",
        "    \"\"\"\n",
        "    # Extract news features\n",
        "    category_cols = [col for col in news_df_encoded.columns if col.startswith('cat_')]\n",
        "    subcategory_cols = [col for col in news_df_encoded.columns if col.startswith('subcat_')]\n",
        "    \n",
        "    # Combine news features\n",
        "    news_features = pd.concat([\n",
        "        news_df_encoded[['NewsId']],\n",
        "        news_df_encoded[category_cols],\n",
        "        news_df_encoded[subcategory_cols]\n",
        "    ], axis=1).set_index('NewsId')\n",
        "    \n",
        "    interactions = []\n",
        "    \n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        # Extract time features\n",
        "        time_features = extract_time_features(row['Time'])\n",
        "        context = {\n",
        "            'history_len': len(row['History'].split()) if pd.notna(row['History']) else 0,\n",
        "            **time_features  # Include time features\n",
        "        }\n",
        "        \n",
        "        # Process impressions\n",
        "        impressions = row['Impressions'].split()\n",
        "        slate = []\n",
        "        rewards = []\n",
        "        \n",
        "        for imp in impressions:\n",
        "            news_id, click = imp.split('-')\n",
        "            if news_id in news_features.index:  # Only include if news exists\n",
        "                slate.append(news_id)\n",
        "                rewards.append(int(click))\n",
        "        \n",
        "        if slate:  # Only include if there are valid articles\n",
        "            interactions.append({\n",
        "                'user_id': row['UserID'],\n",
        "                'context': context,\n",
        "                'slate': slate,\n",
        "                'rewards': rewards,\n",
        "                'news_features': news_features.loc[slate].to_dict('records')\n",
        "            })\n",
        "    \n",
        "    return interactions, news_features\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_vector(context, news_feat):\n",
        "    \"\"\"\n",
        "    Combine context and news features into a single vector\n",
        "    \n",
        "    Args:\n",
        "        context: dictionary with context features\n",
        "        news_feat: dictionary with news features\n",
        "        \n",
        "    Returns:\n",
        "        numpy.array: combined feature vector\n",
        "    \"\"\"\n",
        "    context_vec = np.array([\n",
        "        context['history_len'],\n",
        "        context['hour'],\n",
        "        context['day_of_week'],\n",
        "        context['is_weekend']\n",
        "    ])\n",
        "    \n",
        "    # Convert news features to array, excluding NewsId if present\n",
        "    news_vec = np.array([v for k, v in news_feat.items() if k != 'NewsId'])\n",
        "    \n",
        "    return np.concatenate([context_vec, news_vec])\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_feature_scaler(interactions, prepare_features_fn):\n",
        "    \"\"\"\n",
        "    Create and fit StandardScaler for feature vectors\n",
        "    \n",
        "    Args:\n",
        "        interactions: list of interaction dictionaries\n",
        "        prepare_features_fn: function to prepare feature vectors\n",
        "        \n",
        "    Returns:\n",
        "        StandardScaler: fitted scaler for feature vectors\n",
        "    \"\"\"\n",
        "    all_feature_vectors = []\n",
        "    \n",
        "    for interaction in interactions:\n",
        "        context = interaction['context']\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_fn(context, news_feat)\n",
        "            all_feature_vectors.append(features)\n",
        "    \n",
        "    # Fit the scaler on all possible feature combinations\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(np.array(all_feature_vectors))\n",
        "    \n",
        "    return scaler\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Logistic Regression Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_logistic_regression(X_train, X_test, y_train, y_test, random_state=42):\n",
        "    \"\"\"\n",
        "    Train and evaluate logistic regression model\n",
        "    \n",
        "    Args:\n",
        "        X_train: training features\n",
        "        X_test: test features\n",
        "        y_train: training labels\n",
        "        y_test: test labels\n",
        "        random_state: random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        dict: performance metrics\n",
        "        LogisticRegression: trained model\n",
        "    \"\"\"\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Train model\n",
        "    lr_model = LogisticRegression(random_state=random_state)\n",
        "    lr_model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = lr_model.predict(X_test_scaled)\n",
        "    \n",
        "    results = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred),\n",
        "        'f1': f1_score(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    print(\"LR Model Performance:\")\n",
        "    for metric, value in results.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    return results, lr_model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## LinUCB Bandit Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinUCB:\n",
        "    def __init__(self, d, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Linear Upper Confidence Bound algorithm for contextual bandits\n",
        "        \n",
        "        Args:\n",
        "            d: dimension of feature vectors\n",
        "            alpha: exploration parameter\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        self.A = np.identity(d)\n",
        "        self.b = np.zeros(d)\n",
        "        self.theta = np.zeros(d)\n",
        "\n",
        "        # tracking metrics\n",
        "        self.total_regret = 0\n",
        "        self.cumulative_regret = []\n",
        "        \n",
        "    def get_action(self, context_features, actions_features_scaled):\n",
        "        \"\"\"\n",
        "        Select action using LinUCB\n",
        "        \n",
        "        Args:\n",
        "            context_features: context features\n",
        "            actions_features_scaled: list of pre-scaled feature vectors\n",
        "            \n",
        "        Returns:\n",
        "            int: index of selected action\n",
        "        \"\"\"\n",
        "        A_inv = np.linalg.inv(self.A)\n",
        "        self.theta = A_inv.dot(self.b)\n",
        "        \n",
        "        # Compute UCB for each action\n",
        "        ucb_scores = []\n",
        "        \n",
        "        for x in actions_features_scaled:\n",
        "            # Compute UCB score\n",
        "            mu = x.dot(self.theta)\n",
        "            sigma = np.sqrt(x.dot(A_inv).dot(x))\n",
        "            ucb = mu + self.alpha * sigma\n",
        "            \n",
        "            ucb_scores.append(float(ucb))\n",
        "            \n",
        "        return np.argmax(ucb_scores)\n",
        "    \n",
        "    def update(self, features_scaled, reward, optimal_reward):\n",
        "        \"\"\"\n",
        "        Update model with observed reward\n",
        "        \n",
        "        Args:\n",
        "            features_scaled: scaled feature vector of chosen action\n",
        "            reward: observed reward\n",
        "            optimal_reward: optimal reward that could have been achieved\n",
        "        \"\"\"\n",
        "        self.A += np.outer(features_scaled, features_scaled)\n",
        "        self.b += features_scaled * reward\n",
        "\n",
        "        # Update regret\n",
        "        regret = optimal_reward - reward\n",
        "        self.total_regret += regret\n",
        "        self.cumulative_regret.append(self.total_regret)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_linucb(bandit_model, train_data, scaler, logger=None):\n",
        "    \"\"\"\n",
        "    Train LinUCB model on training data\n",
        "    \n",
        "    Args:\n",
        "        bandit_model: LinUCB model instance\n",
        "        train_data: training data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        LinUCB: trained model\n",
        "    \"\"\"\n",
        "    global_step = 0\n",
        "\n",
        "    for interaction in train_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction\n",
        "        chosen_idx = bandit_model.get_action(\n",
        "            list(context.values()),\n",
        "            action_features_scaled\n",
        "        )\n",
        "        \n",
        "        reward = rewards[chosen_idx]\n",
        "        optimal_reward = max(rewards)\n",
        "        # Update model with scaled features\n",
        "        bandit_model.update(\n",
        "            action_features_scaled[chosen_idx],\n",
        "            reward,\n",
        "            optimal_reward\n",
        "        )\n",
        "\n",
        "        # log metrics\n",
        "        if logger is not None:\n",
        "            logger.log_step(\n",
        "                algorithm='Contextual Bandit',\n",
        "                timestep=global_step,\n",
        "                reward=reward\n",
        "            )\n",
        "        \n",
        "        global_step += 1\n",
        "    \n",
        "    return bandit_model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_linucb(model, eval_data, scaler):\n",
        "    \"\"\"\n",
        "    Evaluate LinUCB model on test data\n",
        "    \n",
        "    Args:\n",
        "        model: trained LinUCB model\n",
        "        eval_data: evaluation data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        float: accuracy of model\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for interaction in eval_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction\n",
        "        pred_idx = model.get_action(\n",
        "            list(context.values()),\n",
        "            action_features_scaled\n",
        "        )\n",
        "        \n",
        "        # Check if prediction matches clicked article\n",
        "        if rewards[pred_idx] == 1:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "    return correct / total\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a72959b",
      "metadata": {},
      "source": [
        "# ## Thompson Sampling Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7fef8189",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ThompsonEpsilonGreedy:\n",
        "    def __init__(self, d, epsilon=0.1, v=1.0, lambda_prior=1.0):\n",
        "        \"\"\"\n",
        "        Thompson Sampling with epsilon-greedy exploration for contextual bandits\n",
        "        \n",
        "        Args:\n",
        "            d: dimension of feature vectors\n",
        "            epsilon: probability of random exploration\n",
        "            v: variance parameter for Gaussian prior\n",
        "            lambda_prior: regularization parameter\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.epsilon = epsilon\n",
        "        self.v = v\n",
        "        \n",
        "        # Prior parameters for Bayesian linear regression\n",
        "        self.B = lambda_prior * np.identity(d)  # Precision matrix\n",
        "        self.mu = np.zeros(d)  # Mean vector\n",
        "        self.f = np.zeros(d)  # Weighted sum of features\n",
        "        \n",
        "        # Tracking metrics\n",
        "        self.total_regret = 0\n",
        "        self.cumulative_regret = []\n",
        "        self.theta_samples = []  # Track parameter samples for visualization\n",
        "        \n",
        "    def sample_theta(self):\n",
        "        \"\"\"Sample model parameters from posterior distribution\"\"\"\n",
        "        cov = np.linalg.inv(self.B)\n",
        "        return np.random.multivariate_normal(self.mu, self.v * cov)\n",
        "    \n",
        "    def get_action(self, context_features, actions_features_scaled):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy Thompson Sampling\n",
        "        \n",
        "        Args:\n",
        "            context_features: context features\n",
        "            actions_features_scaled: list of pre-scaled feature vectors\n",
        "            \n",
        "        Returns:\n",
        "            int: index of selected action\n",
        "        \"\"\"\n",
        "        # Epsilon-greedy exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(len(actions_features_scaled))\n",
        "        \n",
        "        # Thompson Sampling\n",
        "        theta = self.sample_theta()\n",
        "        self.theta_samples.append(theta)  # Store for visualization\n",
        "        \n",
        "        # Compute expected rewards for each action\n",
        "        expected_rewards = []\n",
        "        for x in actions_features_scaled:\n",
        "            reward = x.dot(theta)\n",
        "            expected_rewards.append(float(reward))\n",
        "            \n",
        "        return np.argmax(expected_rewards)\n",
        "    \n",
        "    def update(self, features_scaled, reward, optimal_reward):\n",
        "        \"\"\"\n",
        "        Update posterior distribution with observed reward\n",
        "        \n",
        "        Args:\n",
        "            features_scaled: scaled feature vector of chosen action\n",
        "            reward: observed reward\n",
        "            optimal_reward: optimal reward that could have been achieved\n",
        "        \"\"\"\n",
        "        # Update precision matrix\n",
        "        self.B += np.outer(features_scaled, features_scaled)\n",
        "        \n",
        "        # Update weighted sum of features\n",
        "        self.f += features_scaled * reward\n",
        "        \n",
        "        # Update mean vector\n",
        "        self.mu = np.linalg.solve(self.B, self.f)\n",
        "        \n",
        "        # Update regret\n",
        "        regret = optimal_reward - reward\n",
        "        self.total_regret += regret\n",
        "        self.cumulative_regret.append(self.total_regret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5bf4bea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_thompson(bandit_model, train_data, scaler, logger=None):\n",
        "    \"\"\"\n",
        "    Train Thompson Sampling model on training data\n",
        "    \n",
        "    Args:\n",
        "        bandit_model: Thompson Sampling model instance\n",
        "        train_data: training data in bandit format\n",
        "        scaler: feature scaler\n",
        "        logger: optional metrics logger\n",
        "        \n",
        "    Returns:\n",
        "        ThompsonEpsilonGreedy: trained model\n",
        "    \"\"\"\n",
        "    global_step = 0\n",
        "    \n",
        "    for interaction in train_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction\n",
        "        chosen_idx = bandit_model.get_action(\n",
        "            list(context.values()),\n",
        "            action_features_scaled\n",
        "        )\n",
        "        \n",
        "        reward = rewards[chosen_idx]\n",
        "        optimal_reward = max(rewards)\n",
        "        \n",
        "        # Update model with scaled features\n",
        "        bandit_model.update(\n",
        "            action_features_scaled[chosen_idx],\n",
        "            reward,\n",
        "            optimal_reward\n",
        "        )\n",
        "        \n",
        "        # Log metrics\n",
        "        if logger is not None:\n",
        "            logger.log_step(\n",
        "                algorithm='Thompson-Epsilon',\n",
        "                timestep=global_step,\n",
        "                reward=reward\n",
        "            )\n",
        "        \n",
        "        global_step += 1\n",
        "    \n",
        "    return bandit_model\n",
        "\n",
        "def evaluate_thompson(model, eval_data, scaler):\n",
        "    \"\"\"\n",
        "    Evaluate Thompson Sampling model\n",
        "    \n",
        "    Args:\n",
        "        model: trained Thompson Sampling model\n",
        "        eval_data: evaluation data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        float: accuracy of model\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for interaction in eval_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction (use multiple samples for evaluation)\n",
        "        predictions = []\n",
        "        n_samples = 10\n",
        "        for _ in range(n_samples):\n",
        "            pred_idx = model.get_action(\n",
        "                list(context.values()),\n",
        "                action_features_scaled\n",
        "            )\n",
        "            predictions.append(pred_idx)\n",
        "        \n",
        "        # Use majority vote\n",
        "        pred_idx = max(set(predictions), key=predictions.count)\n",
        "        \n",
        "        # Check if prediction matches clicked article\n",
        "        if rewards[pred_idx] == 1:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    \n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## DQN Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Deep Q-Network architecture\n",
        "        \n",
        "        Args:\n",
        "            input_dim: dimension of input features\n",
        "        \"\"\"\n",
        "        super(DQNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Single output for Q-value\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Experience replay buffer for DQN\n",
        "        \n",
        "        Args:\n",
        "            capacity: maximum buffer size\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        \n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Add experience to buffer\"\"\"\n",
        "        self.buffer.append((\n",
        "            np.array(state, dtype=np.float32),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, dtype=np.float32)\n",
        "        ))\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Return batch as numpy arrays\"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "        \n",
        "        return (\n",
        "            np.array(states, dtype=np.float32),  # [batch_size, state_dim]\n",
        "            np.array(actions, dtype=np.int64),    # [batch_size]\n",
        "            np.array(rewards, dtype=np.float32),  # [batch_size]\n",
        "            np.array(next_states, dtype=np.float32)  # [batch_size, state_dim]\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        DQN Agent for news recommendation\n",
        "        \n",
        "        Args:\n",
        "            state_dim: dimension of state features\n",
        "            device: device to run model on\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.state_dim = state_dim\n",
        "        \n",
        "        # Networks\n",
        "        self.policy_net = DQNetwork(state_dim).to(device)\n",
        "        self.target_net = DQNetwork(state_dim).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        # Training parameters\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.target_update = 10\n",
        "        self.steps = 0\n",
        "        \n",
        "    def select_action(self, state_features_list):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy\n",
        "        \n",
        "        Args:\n",
        "            state_features_list: list of feature vectors for each article\n",
        "            \n",
        "        Returns:\n",
        "            int: index of selected action\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(len(state_features_list))\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            q_values = []\n",
        "            for features in state_features_list:\n",
        "                state = torch.FloatTensor(features).to(self.device)\n",
        "                q_value = self.policy_net(state)\n",
        "                q_values.append(q_value.item())\n",
        "            return np.argmax(q_values)\n",
        "    \n",
        "    def update(self, batch_size):\n",
        "        \"\"\"\n",
        "        Update DQN with batch from replay buffer\n",
        "        \n",
        "        Args:\n",
        "            batch_size: number of samples to use for update\n",
        "            \n",
        "        Returns:\n",
        "            float: loss value\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.from_numpy(states).to(self.device)\n",
        "        rewards = torch.from_numpy(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).to(self.device)\n",
        "        \n",
        "        # Compute Q(s_t, a)\n",
        "        current_q_values = self.policy_net(states)\n",
        "        \n",
        "        # Compute V(s_{t+1}) for all next states\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states)\n",
        "            \n",
        "        # Compute expected Q values\n",
        "        expected_q_values = rewards + (self.gamma * next_q_values)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = self.criterion(current_q_values, expected_q_values)\n",
        "        \n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.steps += 1\n",
        "        if self.steps % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "            \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9d794fca",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update(self, batch_size):\n",
        "    \"\"\"\n",
        "    Update DQN with batch from replay buffer\n",
        "    \n",
        "    Args:\n",
        "        batch_size: number of samples to use for update\n",
        "        \n",
        "    Returns:\n",
        "        float: loss value\n",
        "    \"\"\"\n",
        "    if len(self.replay_buffer) < batch_size:\n",
        "        return\n",
        "    \n",
        "    # Sample batch\n",
        "    states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    states = torch.from_numpy(states).to(self.device)\n",
        "    actions = torch.from_numpy(actions).long().to(self.device)\n",
        "    rewards = torch.from_numpy(rewards).float().unsqueeze(1).to(self.device)\n",
        "    next_states = torch.from_numpy(next_states).to(self.device)\n",
        "    \n",
        "    # Compute Q(s_t, a) - only for the action taken\n",
        "    current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "    \n",
        "    # Compute max_a' Q(s_{t+1}, a') for all next states\n",
        "    with torch.no_grad():\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "        \n",
        "    future_discount = 0.0\n",
        "    \n",
        "    # Compute expected Q values (structure intact for future expansion)\n",
        "    expected_q_values = rewards + (future_discount * next_q_values)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = self.criterion(current_q_values, expected_q_values)\n",
        "    \n",
        "    # Optimize the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    \n",
        "    # Update target network\n",
        "    self.steps += 1\n",
        "    if self.steps % self.target_update == 0:\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "    # Decay epsilon\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(agent, train_data, scaler, num_epochs=5, logger=None):\n",
        "    \"\"\"\n",
        "    Train DQN agent\n",
        "    \n",
        "    Args:\n",
        "        agent: DQN agent\n",
        "        train_data: training data in bandit format\n",
        "        scaler: feature scaler\n",
        "        num_epochs: number of training epochs\n",
        "        \n",
        "    Returns:\n",
        "        tuple: training losses and accuracies\n",
        "    \"\"\"\n",
        "    training_losses = []\n",
        "    accuracies = []\n",
        "    global_step = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        epoch_losses = []\n",
        "        \n",
        "        for interaction in train_data:\n",
        "            context = interaction['context']\n",
        "            rewards = interaction['rewards']\n",
        "            \n",
        "            # Prepare state features for each article\n",
        "            state_features = []\n",
        "            for news_feat in interaction['news_features']:\n",
        "                features = prepare_features_vector(context, news_feat)\n",
        "                features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "                state_features.append(features_scaled)\n",
        "            \n",
        "            # Select action\n",
        "            state_features = np.array(state_features, dtype=np.float32)\n",
        "            action = agent.select_action(state_features)\n",
        "            \n",
        "            # Get reward and update metrics\n",
        "            reward = rewards[action]\n",
        "            if reward == 1:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "            \n",
        "            # Store transition in replay buffer\n",
        "            state = state_features[action]\n",
        "            next_state = state  # Terminal state, so same as current\n",
        "            agent.replay_buffer.push(state, action, reward, next_state)\n",
        "            \n",
        "            # Update network\n",
        "            loss = None\n",
        "            if len(agent.replay_buffer) >= agent.batch_size:\n",
        "                loss = agent.update(agent.batch_size)\n",
        "                if loss is not None:\n",
        "                    epoch_losses.append(loss)\n",
        "\n",
        "            if logger is not None:\n",
        "                logger.log_step(\n",
        "                    algorithm='DQN',\n",
        "                    timestep=global_step,\n",
        "                    reward=reward,\n",
        "                    loss=loss,\n",
        "                    exploration_rate=agent.epsilon\n",
        "                )\n",
        "            \n",
        "            global_step += 1\n",
        "        \n",
        "        # Calculate metrics\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "        epoch_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
        "        \n",
        "        accuracies.append(epoch_accuracy)\n",
        "        training_losses.append(epoch_loss)\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Accuracy: {epoch_accuracy:.4f}\")\n",
        "        print(f\"Average Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"Epsilon: {agent.epsilon:.4f}\")\n",
        "        print(\"---\")\n",
        "    \n",
        "    return training_losses, accuracies\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_dqn(agent, eval_data, scaler):\n",
        "    \"\"\"\n",
        "    Evaluate DQN agent\n",
        "    \n",
        "    Args:\n",
        "        agent: trained DQN agent\n",
        "        eval_data: evaluation data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        float: accuracy of model\n",
        "    \"\"\"\n",
        "    agent.policy_net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for interaction in eval_data:\n",
        "            context = interaction['context']\n",
        "            rewards = interaction['rewards']\n",
        "            \n",
        "            # Prepare state features\n",
        "            state_features = []\n",
        "            for news_feat in interaction['news_features']:\n",
        "                features = prepare_features_vector(context, news_feat)\n",
        "                features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "                state_features.append(features_scaled)\n",
        "            \n",
        "            # Get model prediction (no epsilon-greedy during evaluation)\n",
        "            q_values = []\n",
        "            for features in state_features:\n",
        "                state = torch.FloatTensor(features).to(agent.device)\n",
        "                q_value = agent.policy_net(state)\n",
        "                q_values.append(q_value.item())\n",
        "            \n",
        "            pred_idx = np.argmax(q_values)\n",
        "            \n",
        "            if rewards[pred_idx] == 1:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    \n",
        "    agent.policy_net.train()\n",
        "    return correct / total\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Main Execution\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x31240a250>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News data shape: (51282, 8)\n",
            "Behaviors data shape: (1570, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "news_df, behaviors_df = load_data(sample_fraction=0.01, random_state=42)\n",
        "\n",
        "# Display data overview\n",
        "print(\"News data shape:\", news_df.shape)\n",
        "print(\"Behaviors data shape:\", behaviors_df.shape)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### Logistic Regression Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original size: 58161, Balanced size: 4830\n",
            "Class distribution after balancing:\n",
            "click\n",
            "1    2415\n",
            "0    2415\n",
            "Name: count, dtype: int64\n",
            "LR Model Performance:\n",
            "accuracy: 0.5631\n",
            "precision: 0.5524\n",
            "recall: 0.6172\n",
            "f1: 0.5830\n"
          ]
        }
      ],
      "source": [
        "# Process data for logistic regression\n",
        "news_df_encoded = encode_categorical_features(news_df)\n",
        "interactions_df = process_impressions_for_classification(behaviors_df, news_df_encoded)\n",
        "features_df = prepare_features_for_classification(interactions_df, news_df_encoded)\n",
        "\n",
        "# Prepare features and labels\n",
        "one_hot_cols = [col for col in features_df.columns if col.startswith('cat_') or col.startswith('subcat_')]\n",
        "base_context_cols = ['history_len', 'hour', 'day_of_week', 'is_weekend']\n",
        "X = features_df[base_context_cols + one_hot_cols]\n",
        "y = features_df['click']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate logistic regression\n",
        "lr_results, lr_model = train_and_evaluate_logistic_regression(X_train, X_test, y_train, y_test)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "651e00b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize logger\n",
        "logger = MetricsLogger(log_interval=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### LinUCB Bandit Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LinUCB Test Accuracy: 0.0955\n"
          ]
        }
      ],
      "source": [
        "# Process data for bandit model\n",
        "bandit_data, news_features = prepare_data_for_bandit_and_dqn(behaviors_df, news_df_encoded)\n",
        "\n",
        "# Create and fit feature scaler\n",
        "bandit_scaler = create_feature_scaler(bandit_data, prepare_features_vector)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(bandit_data))\n",
        "bandit_train_data = bandit_data[:train_size]\n",
        "bandit_test_data = bandit_data[train_size:]\n",
        "\n",
        "# Initialize and train LinUCB\n",
        "feature_dim = 4 + len(news_features.columns)  # 4 context features + one-hot features\n",
        "bandit_model = LinUCB(d=feature_dim, alpha=1.0)\n",
        "bandit_model = train_linucb(bandit_model, bandit_train_data, bandit_scaler, logger=logger)\n",
        "\n",
        "# Evaluate LinUCB\n",
        "bandit_accuracy = evaluate_linucb(bandit_model, bandit_test_data, bandit_scaler)\n",
        "print(f\"LinUCB Test Accuracy: {bandit_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ad0aaf",
      "metadata": {},
      "source": [
        "# ### Thompson Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "21d94a13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thompson-Epsilon Test Accuracy: 0.1115\n"
          ]
        }
      ],
      "source": [
        "# Initialize Thompson Sampling model\n",
        "thompson_model = ThompsonEpsilonGreedy(\n",
        "    d=feature_dim,\n",
        "    epsilon=0.1,\n",
        "    v=1.0,\n",
        "    lambda_prior=10.0\n",
        ")\n",
        "\n",
        "# Train model\n",
        "thompson_model = train_thompson(thompson_model, bandit_train_data, bandit_scaler)\n",
        "\n",
        "# Evaluate model\n",
        "thompson_accuracy = evaluate_thompson(thompson_model, bandit_test_data, bandit_scaler)\n",
        "print(f\"Thompson-Epsilon Test Accuracy: {thompson_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### DQN Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Accuracy: 0.1314\n",
            "Average Loss: 0.1246\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 2/5\n",
            "Accuracy: 0.1521\n",
            "Average Loss: 0.1728\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 3/5\n",
            "Accuracy: 0.1680\n",
            "Average Loss: 0.1912\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 4/5\n",
            "Accuracy: 0.1696\n",
            "Average Loss: 0.1981\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 5/5\n",
            "Accuracy: 0.1783\n",
            "Average Loss: 0.2302\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "\n",
            "DQN Test Accuracy: 0.1624\n"
          ]
        }
      ],
      "source": [
        "# Reuse the bandit data format for DQN\n",
        "dqn_train_data = bandit_train_data\n",
        "dqn_test_data = bandit_test_data\n",
        "dqn_scaler = bandit_scaler\n",
        "\n",
        "# Initialize DQN agent\n",
        "state_dim = feature_dim  # Same as bandit feature dimension\n",
        "dqn_agent = DQNAgent(state_dim)\n",
        "\n",
        "# Train DQN\n",
        "dqn_losses, dqn_accuracies = train_dqn(dqn_agent, dqn_train_data, dqn_scaler, num_epochs=5, logger=logger)\n",
        "\n",
        "# Evaluate DQN\n",
        "dqn_accuracy = evaluate_dqn(dqn_agent, dqn_test_data, dqn_scaler)\n",
        "print(f\"\\nDQN Test Accuracy: {dqn_accuracy:.4f}\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Model Comparison\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "629b357e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random baseline (1/avg_slate_size): 0.0270\n"
          ]
        }
      ],
      "source": [
        "# Calculate random baseline\n",
        "avg_slate_size = np.mean([len(interaction['slate']) for interaction in bandit_data])\n",
        "random_baseline = 1/avg_slate_size\n",
        "print(f\"Random baseline (1/avg_slate_size): {random_baseline:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compare model performances\n",
        "models = ['Logistic Regression', 'LinUCB Bandit', 'DQN', 'Thompson Sampling']\n",
        "accuracies = [lr_results['accuracy'], bandit_accuracy, dqn_accuracy, thompson_accuracy]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracies)\n",
        "plt.axhline(y=random_baseline, color='r', linestyle='-', label='Random Baseline')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8419411a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # After training both models, plot the comparison\n",
        "# plot_comparison(logger, save_path='dqn_vs_linucb_comparison.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
