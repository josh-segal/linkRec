{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b747ada",
      "metadata": {},
      "source": [
        "# ## Data Class for Logging Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003968a1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Data Loading Functions\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(sample_fraction=0.01, random_state=42):\n",
        "    \"\"\"\n",
        "    Load news and behaviors data from MIND dataset with optional sampling\n",
        "    \n",
        "    Args:\n",
        "        sample_fraction: fraction of behaviors data to sample (default: 0.01)\n",
        "        random_state: random seed for reproducibility (default: 42)\n",
        "        \n",
        "    Returns:\n",
        "        news_df: DataFrame containing news articles\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "    \"\"\"\n",
        "    # Read news data\n",
        "    col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
        "    news_df = pd.read_csv('MINDsmall_train/news.tsv', sep='\\t', header=None, names=col_news)\n",
        "    \n",
        "    # Read behaviors data\n",
        "    col_behaviors = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
        "    behaviors_df = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None, names=col_behaviors)\n",
        "    behaviors_df = behaviors_df.sample(frac=sample_fraction, random_state=random_state)\n",
        "    \n",
        "    return news_df, behaviors_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Common Utility Functions\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_time_features(time_str):\n",
        "    \"\"\"\n",
        "    Extract time-based features from timestamp\n",
        "    \n",
        "    Args:\n",
        "        time_str: timestamp string in format '%m/%d/%Y %I:%M:%S %p'\n",
        "        \n",
        "    Returns:\n",
        "        dict: containing hour, day_of_week, is_weekend features\n",
        "    \"\"\"\n",
        "    time = datetime.strptime(time_str, '%m/%d/%Y %I:%M:%S %p')\n",
        "    return {\n",
        "        'hour': time.hour,\n",
        "        'day_of_week': time.weekday(),\n",
        "        'is_weekend': 1 if time.weekday() >= 5 else 0\n",
        "    }\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_categorical_features(news_df):\n",
        "    \"\"\"\n",
        "    One-hot encode news category and subcategory\n",
        "    \n",
        "    Args:\n",
        "        news_df: DataFrame containing news articles\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: news_df with appended one-hot encoded features\n",
        "    \"\"\"\n",
        "    # One-hot encode category and subcategory\n",
        "    category_ohe = pd.get_dummies(news_df['Category'], prefix='cat')\n",
        "    subcategory_ohe = pd.get_dummies(news_df['SubCat'], prefix='subcat')\n",
        "\n",
        "    # Concatenate one-hot columns to news_df\n",
        "    return pd.concat([news_df, category_ohe, subcategory_ohe], axis=1)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Data Processing for Different Models\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_impressions_for_classification(behaviors_df, news_df):\n",
        "    \"\"\"\n",
        "    Convert behaviors data into user-news interaction pairs for classification\n",
        "    with balanced classes through downsampling\n",
        "    \n",
        "    Args:\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "        news_df: DataFrame containing news articles\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: balanced interactions for classification task\n",
        "    \"\"\"\n",
        "    interactions = []\n",
        "    \n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        user_id = row['UserID']\n",
        "        history = row['History'].split() if pd.notna(row['History']) else []\n",
        "        \n",
        "        # Process each impression\n",
        "        for impression in row['Impressions'].split():\n",
        "            news_id, click = impression.split('-')\n",
        "            \n",
        "            # Only include if news exists in news_df\n",
        "            if news_id in news_df['NewsId'].values:\n",
        "                interactions.append({\n",
        "                    'user_id': user_id,\n",
        "                    'news_id': news_id,\n",
        "                    'click': int(click),\n",
        "                    'history_len': len(history),\n",
        "                    'time': row['Time']\n",
        "                })\n",
        "\n",
        "    # Convert interactions list to DataFrame\n",
        "    interactions_df = pd.DataFrame(interactions)\n",
        "    \n",
        "    # Separate clicks and no-clicks\n",
        "    clicks = interactions_df[interactions_df['click'] == 1]\n",
        "    no_clicks = interactions_df[interactions_df['click'] == 0]\n",
        "    \n",
        "    # Downsample no_clicks to match clicks size\n",
        "    no_clicks_downsampled = no_clicks.sample(n=len(clicks), random_state=42)\n",
        "    \n",
        "    # Combine back\n",
        "    balanced_df = pd.concat([clicks, no_clicks_downsampled])\n",
        "    \n",
        "    print(f\"Original size: {len(interactions_df)}, Balanced size: {len(balanced_df)}\")\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    print(balanced_df['click'].value_counts())\n",
        "    \n",
        "    return balanced_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_for_classification(interactions_df, news_df):\n",
        "    \"\"\"\n",
        "    Prepare features for classification task\n",
        "    \n",
        "    Args:\n",
        "        interactions_df: DataFrame with user-news interactions\n",
        "        news_df: DataFrame with one-hot encoded news features\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: features ready for classification\n",
        "    \"\"\"\n",
        "    # Identify one-hot columns to include\n",
        "    category_ohe = [col for col in news_df.columns if col.startswith('cat_')]\n",
        "    subcategory_ohe = [col for col in news_df.columns if col.startswith('subcat_')]\n",
        "\n",
        "    # Merge only necessary columns (include one-hot columns)\n",
        "    merged_news_cols = ['NewsId'] + category_ohe + subcategory_ohe\n",
        "\n",
        "    # Merge news features with interactions\n",
        "    features_df = interactions_df.merge(\n",
        "        news_df[merged_news_cols],\n",
        "        left_on='news_id',\n",
        "        right_on='NewsId'\n",
        "    )\n",
        "    \n",
        "    # Add time features\n",
        "    time_features = features_df['time'].apply(extract_time_features).apply(pd.Series)\n",
        "    features_df = pd.concat([features_df, time_features], axis=1)\n",
        "    \n",
        "    return features_df\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data_for_bandit_and_dqn(behaviors_df, news_df_encoded):\n",
        "    \"\"\"\n",
        "    Process data into format suitable for both LinUCB bandit and DQN models\n",
        "    \n",
        "    Args:\n",
        "        behaviors_df: DataFrame containing user behaviors\n",
        "        news_df_encoded: DataFrame containing news with encoded features\n",
        "        \n",
        "    Returns:\n",
        "        list: interactions in bandit/DQN format\n",
        "        DataFrame: news features for reference\n",
        "    \"\"\"\n",
        "    # Extract news features\n",
        "    category_cols = [col for col in news_df_encoded.columns if col.startswith('cat_')]\n",
        "    subcategory_cols = [col for col in news_df_encoded.columns if col.startswith('subcat_')]\n",
        "    \n",
        "    # Combine news features\n",
        "    news_features = pd.concat([\n",
        "        news_df_encoded[['NewsId']],\n",
        "        news_df_encoded[category_cols],\n",
        "        news_df_encoded[subcategory_cols]\n",
        "    ], axis=1).set_index('NewsId')\n",
        "    \n",
        "    interactions = []\n",
        "    \n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        # Extract time features\n",
        "        time_features = extract_time_features(row['Time'])\n",
        "        context = {\n",
        "            'history_len': len(row['History'].split()) if pd.notna(row['History']) else 0,\n",
        "            **time_features  # Include time features\n",
        "        }\n",
        "        \n",
        "        # Process impressions\n",
        "        impressions = row['Impressions'].split()\n",
        "        slate = []\n",
        "        rewards = []\n",
        "        \n",
        "        for imp in impressions:\n",
        "            news_id, click = imp.split('-')\n",
        "            if news_id in news_features.index:  # Only include if news exists\n",
        "                slate.append(news_id)\n",
        "                rewards.append(int(click))\n",
        "        \n",
        "        if slate:  # Only include if there are valid articles\n",
        "            interactions.append({\n",
        "                'user_id': row['UserID'],\n",
        "                'context': context,\n",
        "                'slate': slate,\n",
        "                'rewards': rewards,\n",
        "                'news_features': news_features.loc[slate].to_dict('records')\n",
        "            })\n",
        "    \n",
        "    return interactions, news_features\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_vector(context, news_feat):\n",
        "    \"\"\"\n",
        "    Combine context and news features into a single vector\n",
        "    \n",
        "    Args:\n",
        "        context: dictionary with context features\n",
        "        news_feat: dictionary with news features\n",
        "        \n",
        "    Returns:\n",
        "        numpy.array: combined feature vector\n",
        "    \"\"\"\n",
        "    context_vec = np.array([\n",
        "        context['history_len'],\n",
        "        context['hour'],\n",
        "        context['day_of_week'],\n",
        "        context['is_weekend']\n",
        "    ])\n",
        "    \n",
        "    # Convert news features to array, excluding NewsId if present\n",
        "    news_vec = np.array([v for k, v in news_feat.items() if k != 'NewsId'])\n",
        "    \n",
        "    return np.concatenate([context_vec, news_vec])\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_feature_scaler(interactions, prepare_features_fn):\n",
        "    \"\"\"\n",
        "    Create and fit StandardScaler for feature vectors\n",
        "    \n",
        "    Args:\n",
        "        interactions: list of interaction dictionaries\n",
        "        prepare_features_fn: function to prepare feature vectors\n",
        "        \n",
        "    Returns:\n",
        "        StandardScaler: fitted scaler for feature vectors\n",
        "    \"\"\"\n",
        "    all_feature_vectors = []\n",
        "    \n",
        "    for interaction in interactions:\n",
        "        context = interaction['context']\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_fn(context, news_feat)\n",
        "            all_feature_vectors.append(features)\n",
        "    \n",
        "    # Fit the scaler on all possible feature combinations\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(np.array(all_feature_vectors))\n",
        "    \n",
        "    return scaler\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Logistic Regression Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_logistic_regression(X_train, X_test, y_train, y_test, random_state=42):\n",
        "    \"\"\"\n",
        "    Train and evaluate logistic regression model\n",
        "    \n",
        "    Args:\n",
        "        X_train: training features\n",
        "        X_test: test features\n",
        "        y_train: training labels\n",
        "        y_test: test labels\n",
        "        random_state: random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        dict: performance metrics\n",
        "        LogisticRegression: trained model\n",
        "    \"\"\"\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Train model\n",
        "    lr_model = LogisticRegression(random_state=random_state)\n",
        "    lr_model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = lr_model.predict(X_test_scaled)\n",
        "    \n",
        "    results = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred),\n",
        "        'f1': f1_score(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    print(\"LR Model Performance:\")\n",
        "    for metric, value in results.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    return results, lr_model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## LinUCB Bandit Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinUCB:\n",
        "    def __init__(self, d, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Linear Upper Confidence Bound algorithm for contextual bandits\n",
        "        \n",
        "        Args:\n",
        "            d: dimension of feature vectors\n",
        "            alpha: exploration parameter\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        self.A = np.identity(d)\n",
        "        self.b = np.zeros(d)\n",
        "        self.theta = np.zeros(d)\n",
        "\n",
        "        # tracking metrics\n",
        "        self.total_regret = 0\n",
        "        self.cumulative_regret = []\n",
        "        \n",
        "    def get_action(self, context_features, actions_features_scaled):\n",
        "        \"\"\"\n",
        "        Select action using LinUCB\n",
        "        \n",
        "        Args:\n",
        "            context_features: context features\n",
        "            actions_features_scaled: list of pre-scaled feature vectors\n",
        "            \n",
        "        Returns:\n",
        "            int: index of selected action\n",
        "        \"\"\"\n",
        "        A_inv = np.linalg.inv(self.A)\n",
        "        self.theta = A_inv.dot(self.b)\n",
        "        \n",
        "        # Compute UCB for each action\n",
        "        ucb_scores = []\n",
        "        \n",
        "        for x in actions_features_scaled:\n",
        "            # Compute UCB score\n",
        "            mu = x.dot(self.theta)\n",
        "            sigma = np.sqrt(x.dot(A_inv).dot(x))\n",
        "            ucb = mu + self.alpha * sigma\n",
        "            \n",
        "            ucb_scores.append(float(ucb))\n",
        "            \n",
        "        return np.argmax(ucb_scores)\n",
        "    \n",
        "    def update(self, features_scaled, reward, optimal_reward):\n",
        "        \"\"\"\n",
        "        Update model with observed reward\n",
        "        \n",
        "        Args:\n",
        "            features_scaled: scaled feature vector of chosen action\n",
        "            reward: observed reward\n",
        "            optimal_reward: optimal reward that could have been achieved\n",
        "        \"\"\"\n",
        "        self.A += np.outer(features_scaled, features_scaled)\n",
        "        self.b += features_scaled * reward\n",
        "\n",
        "        # Update regret\n",
        "        regret = optimal_reward - reward\n",
        "        self.total_regret += regret\n",
        "        self.cumulative_regret.append(self.total_regret)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_linucb(bandit_model, train_data, scaler):\n",
        "    \"\"\"\n",
        "    Train LinUCB model on training data\n",
        "    \n",
        "    Args:\n",
        "        bandit_model: LinUCB model instance\n",
        "        train_data: training data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        LinUCB: trained model\n",
        "    \"\"\"\n",
        "    for interaction in train_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction\n",
        "        chosen_idx = bandit_model.get_action(\n",
        "            list(context.values()),\n",
        "            action_features_scaled\n",
        "        )\n",
        "        \n",
        "        optimal_reward = max(rewards)\n",
        "        # Update model with scaled features\n",
        "        bandit_model.update(\n",
        "            action_features_scaled[chosen_idx],\n",
        "            rewards[chosen_idx],\n",
        "            optimal_reward\n",
        "        )\n",
        "    \n",
        "    return bandit_model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_linucb(model, eval_data, scaler):\n",
        "    \"\"\"\n",
        "    Evaluate LinUCB model on test data\n",
        "    \n",
        "    Args:\n",
        "        model: trained LinUCB model\n",
        "        eval_data: evaluation data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        float: accuracy of model\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for interaction in eval_data:\n",
        "        context = interaction['context']\n",
        "        rewards = interaction['rewards']\n",
        "        \n",
        "        # Prepare and scale features for each article\n",
        "        action_features_scaled = []\n",
        "        for news_feat in interaction['news_features']:\n",
        "            features = prepare_features_vector(context, news_feat)\n",
        "            features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "            action_features_scaled.append(features_scaled)\n",
        "        \n",
        "        # Get model prediction\n",
        "        pred_idx = model.get_action(\n",
        "            list(context.values()),\n",
        "            action_features_scaled\n",
        "        )\n",
        "        \n",
        "        # Check if prediction matches clicked article\n",
        "        if rewards[pred_idx] == 1:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "    return correct / total\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## DQN Model\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Deep Q-Network architecture\n",
        "        \n",
        "        Args:\n",
        "            input_dim: dimension of input features\n",
        "        \"\"\"\n",
        "        super(DQNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Single output for Q-value\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Experience replay buffer for DQN\n",
        "        \n",
        "        Args:\n",
        "            capacity: maximum buffer size\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        \n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Add experience to buffer\"\"\"\n",
        "        self.buffer.append((\n",
        "            np.array(state, dtype=np.float32),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, dtype=np.float32)\n",
        "        ))\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Return batch as numpy arrays\"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "        \n",
        "        return (\n",
        "            np.array(states, dtype=np.float32),  # [batch_size, state_dim]\n",
        "            np.array(actions, dtype=np.int64),    # [batch_size]\n",
        "            np.array(rewards, dtype=np.float32),  # [batch_size]\n",
        "            np.array(next_states, dtype=np.float32)  # [batch_size, state_dim]\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        DQN Agent for news recommendation\n",
        "        \n",
        "        Args:\n",
        "            state_dim: dimension of state features\n",
        "            device: device to run model on\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.state_dim = state_dim\n",
        "        \n",
        "        # Networks\n",
        "        self.policy_net = DQNetwork(state_dim).to(device)\n",
        "        self.target_net = DQNetwork(state_dim).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        # Training parameters\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.target_update = 10\n",
        "        self.steps = 0\n",
        "        \n",
        "    def select_action(self, state_features_list):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy\n",
        "        \n",
        "        Args:\n",
        "            state_features_list: list of feature vectors for each article\n",
        "            \n",
        "        Returns:\n",
        "            int: index of selected action\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(len(state_features_list))\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            q_values = []\n",
        "            for features in state_features_list:\n",
        "                state = torch.FloatTensor(features).to(self.device)\n",
        "                q_value = self.policy_net(state)\n",
        "                q_values.append(q_value.item())\n",
        "            return np.argmax(q_values)\n",
        "    \n",
        "    def update(self, batch_size):\n",
        "        \"\"\"\n",
        "        Update DQN with batch from replay buffer\n",
        "        \n",
        "        Args:\n",
        "            batch_size: number of samples to use for update\n",
        "            \n",
        "        Returns:\n",
        "            float: loss value\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.from_numpy(states).to(self.device)\n",
        "        rewards = torch.from_numpy(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).to(self.device)\n",
        "        \n",
        "        # Compute Q(s_t, a)\n",
        "        current_q_values = self.policy_net(states)\n",
        "        \n",
        "        # Compute V(s_{t+1}) for all next states\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states)\n",
        "            \n",
        "        # Compute expected Q values\n",
        "        expected_q_values = rewards + (self.gamma * next_q_values)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = self.criterion(current_q_values, expected_q_values)\n",
        "        \n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.steps += 1\n",
        "        if self.steps % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "            \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9d794fca",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update(self, batch_size):\n",
        "    \"\"\"\n",
        "    Update DQN with batch from replay buffer\n",
        "    \n",
        "    Args:\n",
        "        batch_size: number of samples to use for update\n",
        "        \n",
        "    Returns:\n",
        "        float: loss value\n",
        "    \"\"\"\n",
        "    if len(self.replay_buffer) < batch_size:\n",
        "        return\n",
        "    \n",
        "    # Sample batch\n",
        "    states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    states = torch.from_numpy(states).to(self.device)\n",
        "    actions = torch.from_numpy(actions).long().to(self.device)\n",
        "    rewards = torch.from_numpy(rewards).float().unsqueeze(1).to(self.device)\n",
        "    next_states = torch.from_numpy(next_states).to(self.device)\n",
        "    \n",
        "    # Compute Q(s_t, a) - only for the action taken\n",
        "    current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "    \n",
        "    # Compute max_a' Q(s_{t+1}, a') for all next states\n",
        "    with torch.no_grad():\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "        \n",
        "    future_discount = 0.0\n",
        "    \n",
        "    # Compute expected Q values (structure intact for future expansion)\n",
        "    expected_q_values = rewards + (future_discount * next_q_values)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = self.criterion(current_q_values, expected_q_values)\n",
        "    \n",
        "    # Optimize the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    \n",
        "    # Update target network\n",
        "    self.steps += 1\n",
        "    if self.steps % self.target_update == 0:\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "    # Decay epsilon\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(agent, train_data, scaler, num_epochs=5):\n",
        "    \"\"\"\n",
        "    Train DQN agent\n",
        "    \n",
        "    Args:\n",
        "        agent: DQN agent\n",
        "        train_data: training data in bandit format\n",
        "        scaler: feature scaler\n",
        "        num_epochs: number of training epochs\n",
        "        \n",
        "    Returns:\n",
        "        tuple: training losses and accuracies\n",
        "    \"\"\"\n",
        "    training_losses = []\n",
        "    accuracies = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        epoch_losses = []\n",
        "        \n",
        "        for interaction in train_data:\n",
        "            context = interaction['context']\n",
        "            rewards = interaction['rewards']\n",
        "            \n",
        "            # Prepare state features for each article\n",
        "            state_features = []\n",
        "            for news_feat in interaction['news_features']:\n",
        "                features = prepare_features_vector(context, news_feat)\n",
        "                features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "                state_features.append(features_scaled)\n",
        "            \n",
        "            # Select action\n",
        "            state_features = np.array(state_features, dtype=np.float32)\n",
        "            action = agent.select_action(state_features)\n",
        "            \n",
        "            # Get reward and update metrics\n",
        "            reward = rewards[action]\n",
        "            if reward == 1:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "            \n",
        "            # Store transition in replay buffer\n",
        "            state = state_features[action]\n",
        "            next_state = state  # Terminal state, so same as current\n",
        "            agent.replay_buffer.push(state, action, reward, next_state)\n",
        "            \n",
        "            # Update network\n",
        "            if len(agent.replay_buffer) >= agent.batch_size:\n",
        "                loss = agent.update(agent.batch_size)\n",
        "                if loss is not None:\n",
        "                    epoch_losses.append(loss)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "        epoch_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
        "        \n",
        "        accuracies.append(epoch_accuracy)\n",
        "        training_losses.append(epoch_loss)\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Accuracy: {epoch_accuracy:.4f}\")\n",
        "        print(f\"Average Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"Epsilon: {agent.epsilon:.4f}\")\n",
        "        print(\"---\")\n",
        "    \n",
        "    return training_losses, accuracies\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_dqn(agent, eval_data, scaler):\n",
        "    \"\"\"\n",
        "    Evaluate DQN agent\n",
        "    \n",
        "    Args:\n",
        "        agent: trained DQN agent\n",
        "        eval_data: evaluation data in bandit format\n",
        "        scaler: feature scaler\n",
        "        \n",
        "    Returns:\n",
        "        float: accuracy of model\n",
        "    \"\"\"\n",
        "    agent.policy_net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for interaction in eval_data:\n",
        "            context = interaction['context']\n",
        "            rewards = interaction['rewards']\n",
        "            \n",
        "            # Prepare state features\n",
        "            state_features = []\n",
        "            for news_feat in interaction['news_features']:\n",
        "                features = prepare_features_vector(context, news_feat)\n",
        "                features_scaled = scaler.transform(features.reshape(1, -1))[0]\n",
        "                state_features.append(features_scaled)\n",
        "            \n",
        "            # Get model prediction (no epsilon-greedy during evaluation)\n",
        "            q_values = []\n",
        "            for features in state_features:\n",
        "                state = torch.FloatTensor(features).to(agent.device)\n",
        "                q_value = agent.policy_net(state)\n",
        "                q_values.append(q_value.item())\n",
        "            \n",
        "            pred_idx = np.argmax(q_values)\n",
        "            \n",
        "            if rewards[pred_idx] == 1:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    \n",
        "    agent.policy_net.train()\n",
        "    return correct / total\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Main Execution\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x14f5021b0>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News data shape: (51282, 8)\n",
            "Behaviors data shape: (15696, 5)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "news_df, behaviors_df = load_data(sample_fraction=0.1, random_state=42)\n",
        "\n",
        "# Display data overview\n",
        "print(\"News data shape:\", news_df.shape)\n",
        "print(\"Behaviors data shape:\", behaviors_df.shape)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### Logistic Regression Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original size: 582274, Balanced size: 47506\n",
            "Class distribution after balancing:\n",
            "click\n",
            "1    23753\n",
            "0    23753\n",
            "Name: count, dtype: int64\n",
            "LR Model Performance:\n",
            "accuracy: 0.5843\n",
            "precision: 0.5813\n",
            "recall: 0.5933\n",
            "f1: 0.5873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Process data for logistic regression\n",
        "news_df_encoded = encode_categorical_features(news_df)\n",
        "interactions_df = process_impressions_for_classification(behaviors_df, news_df_encoded)\n",
        "features_df = prepare_features_for_classification(interactions_df, news_df_encoded)\n",
        "\n",
        "# Prepare features and labels\n",
        "one_hot_cols = [col for col in features_df.columns if col.startswith('cat_') or col.startswith('subcat_')]\n",
        "base_context_cols = ['history_len', 'hour', 'day_of_week', 'is_weekend']\n",
        "X = features_df[base_context_cols + one_hot_cols]\n",
        "y = features_df['click']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate logistic regression\n",
        "lr_results, lr_model = train_and_evaluate_logistic_regression(X_train, X_test, y_train, y_test)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### LinUCB Bandit Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LinUCB Test Accuracy: 0.1248\n",
            "Random baseline (1/avg_slate_size): 0.0270\n"
          ]
        }
      ],
      "source": [
        "# Process data for bandit model\n",
        "bandit_data, news_features = prepare_data_for_bandit_and_dqn(behaviors_df, news_df_encoded)\n",
        "\n",
        "# Create and fit feature scaler\n",
        "bandit_scaler = create_feature_scaler(bandit_data, prepare_features_vector)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(bandit_data))\n",
        "bandit_train_data = bandit_data[:train_size]\n",
        "bandit_test_data = bandit_data[train_size:]\n",
        "\n",
        "# Initialize and train LinUCB\n",
        "feature_dim = 4 + len(news_features.columns)  # 4 context features + one-hot features\n",
        "bandit_model = LinUCB(d=feature_dim, alpha=1.0)\n",
        "bandit_model = train_linucb(bandit_model, bandit_train_data, bandit_scaler)\n",
        "\n",
        "# Evaluate LinUCB\n",
        "bandit_accuracy = evaluate_linucb(bandit_model, bandit_test_data, bandit_scaler)\n",
        "print(f\"LinUCB Test Accuracy: {bandit_accuracy:.4f}\")\n",
        "\n",
        "# Calculate random baseline\n",
        "avg_slate_size = np.mean([len(interaction['slate']) for interaction in bandit_data])\n",
        "random_baseline = 1/avg_slate_size\n",
        "print(f\"Random baseline (1/avg_slate_size): {random_baseline:.4f}\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### DQN Pipeline\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Accuracy: 0.1455\n",
            "Average Loss: 0.1840\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 2/5\n",
            "Accuracy: 0.1559\n",
            "Average Loss: 0.1890\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 3/5\n",
            "Accuracy: 0.1642\n",
            "Average Loss: 0.1900\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 4/5\n",
            "Accuracy: 0.1689\n",
            "Average Loss: 0.2069\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "Epoch 5/5\n",
            "Accuracy: 0.1705\n",
            "Average Loss: 0.2046\n",
            "Epsilon: 0.0100\n",
            "---\n",
            "\n",
            "DQN Test Accuracy: 0.1739\n",
            "Random baseline (1/avg_slate_size): 0.0270\n"
          ]
        }
      ],
      "source": [
        "# Reuse the bandit data format for DQN\n",
        "dqn_train_data = bandit_train_data\n",
        "dqn_test_data = bandit_test_data\n",
        "dqn_scaler = bandit_scaler\n",
        "\n",
        "# Initialize DQN agent\n",
        "state_dim = feature_dim  # Same as bandit feature dimension\n",
        "dqn_agent = DQNAgent(state_dim)\n",
        "\n",
        "# Train DQN\n",
        "dqn_losses, dqn_accuracies = train_dqn(dqn_agent, dqn_train_data, dqn_scaler, num_epochs=5)\n",
        "\n",
        "# Evaluate DQN\n",
        "dqn_accuracy = evaluate_dqn(dqn_agent, dqn_test_data, dqn_scaler)\n",
        "print(f\"\\nDQN Test Accuracy: {dqn_accuracy:.4f}\")\n",
        "print(f\"Random baseline (1/avg_slate_size): {random_baseline:.4f}\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## Model Comparison\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compare model performances\n",
        "models = ['Logistic Regression', 'LinUCB Bandit', 'DQN']\n",
        "accuracies = [lr_results['accuracy'], bandit_accuracy, dqn_accuracy]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracies)\n",
        "plt.axhline(y=random_baseline, color='r', linestyle='-', label='Random Baseline')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
